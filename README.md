# üß† Building Perceptron

> *"The perceptron is capable of generalization and abstraction; it may recognize similarities between patterns which are not identical."* - Frank Rosenblatt

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3.0-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Un projet complet d'initiation au Deep Learning, impl√©mentant le Perceptron de Rosenblatt from scratch et l'appliquant au diagnostic du cancer du sein.

---

## üìã Table des mati√®res

- [Contexte du projet](#-contexte-du-projet)
- [Objectifs](#-objectifs)
- [Dataset](#-dataset)
- [M√©thodologie](#-m√©thodologie)
- [Outils et technologies](#-outils-et-technologies)
- [Structure du projet](#-structure-du-projet)
- [Installation et utilisation](#-installation-et-utilisation)
- [R√©sultats](#-r√©sultats)
- [Limites et am√©liorations](#-limites-et-am√©liorations)
- [Bibliographie](#-bibliographie)

---

## üéØ Contexte du projet

L'intelligence artificielle s'impose progressivement dans notre quotidien, enrichissant notre vocabulaire de termes parfois d√©concertants : **Machine Learning**, **Deep Learning**, **r√©seaux de neurones**. Ce projet explore les fondations historiques du Deep Learning moderne en impl√©mentant et testant le **Perceptron**, le premier neurone artificiel invent√© par Frank Rosenblatt en 1957.

Le projet s'inscrit dans un parcours de formation en Data Science et vise √† :
- Comprendre les concepts fondamentaux du Machine Learning et du Deep Learning
- Impl√©menter from scratch un algorithme d'apprentissage supervis√©
- Appliquer des techniques rigoureuses d'analyse exploratoire et de pr√©traitement
- √âvaluer un mod√®le de classification binaire sur un probl√®me r√©el

---

## üéì Objectifs

### Objectifs th√©oriques
1. D√©finir et comparer Machine Learning et Deep Learning
2. Explorer les applications modernes du Deep Learning
3. Comprendre le fonctionnement math√©matique du Perceptron
4. √âtudier l'analogie entre neurones biologiques et artificiels
5. Analyser les limites du Perceptron et les solutions modernes

### Objectifs pratiques
1. Impl√©menter un Perceptron en programmation orient√©e objet (Python)
2. R√©aliser une analyse exploratoire compl√®te (EDA)
3. Appliquer des techniques de pr√©traitement (normalisation)
4. R√©duire la dimensionnalit√© (PCA)
5. Entra√Æner et √©valuer le mod√®le avec des m√©triques adapt√©es
6. Proposer des am√©liorations pertinentes

---

## üìä Dataset

### Breast Cancer Wisconsin (Diagnostic)

Le dataset utilis√© est le **Breast Cancer Wisconsin (Diagnostic)**, disponible dans scikit-learn. Il s'agit d'un dataset classique de classification binaire dans le domaine m√©dical.

**Caract√©ristiques** :
- **569 √©chantillons** : tumeurs mammaires (212 malignes, 357 b√©nignes)
- **30 features** : mesures morphologiques des cellules tumorales
- **2 classes** : Malignant (canc√©reux) / Benign (non canc√©reux)

**Features** : Pour chaque tumeur, 10 caract√©ristiques morphologiques ont √©t√© mesur√©es (rayon, texture, p√©rim√®tre, surface, rugosit√©, compacit√©, concavit√©, points concaves, sym√©trie, dimension fractale), et pour chacune, 3 statistiques sont fournies (moyenne, erreur standard, pire valeur), donnant 30 features au total.

**Source** : William H. Wolberg, W. Nick Street, Olvi L. Mangasarian (1995)

**D√©s√©quilibre des classes** : Ratio 1.7:1 (benign:malignant), d√©s√©quilibre mod√©r√© ne n√©cessitant pas de techniques de r√©√©chantillonnage.

---

## üî¨ M√©thodologie

### 1. **Introduction th√©orique**
   - D√©finitions ML vs DL
   - Applications concr√®tes du Deep Learning (GPT-4, DALL-E, Quick Draw!)
   - Pr√©sentation du Perceptron de Rosenblatt

### 2. **Chargement et exploration des donn√©es**
   - Chargement du dataset Breast Cancer Wisconsin
   - V√©rification de l'int√©grit√© (valeurs manquantes, types de donn√©es)
   - Statistiques descriptives

### 3. **Analyse exploratoire (EDA)**
   - Distribution de la variable cible
   - Analyse univari√©e des features
   - Analyse bivari√©e (comparaison par diagnostic)
   - Matrice de corr√©lation (identification de la multicolin√©arit√©)
   - D√©tection des outliers
   - **Insights** : Features tr√®s corr√©l√©es (radius/perimeter/area), s√©paration visible entre classes

### 4. **Pr√©processing**
   - Normalisation avec StandardScaler (mean=0, std=1)
   - **Justification** : Le perceptron est sensible √† l'√©chelle des features
   
### 5. **R√©duction de dimensionnalit√© (PCA)**
   - Application de la PCA pour conserver 95% de la variance
   - **R√©sultat** : R√©duction de 30 features √† ~10 composantes principales
   - Visualisation en 2D : s√©paration relativement lin√©aire des classes
   - **Justification** : Multicolin√©arit√© forte + mal√©diction de la dimensionnalit√©

### 6. **Mod√©lisation**
   - Split train/test (80/20) avec stratification
   - Entra√Ænement du Perceptron (learning_rate=0.01, epochs=100)
   - Visualisation de la convergence

### 7. **√âvaluation**
   - M√©triques : Accuracy, Precision, Recall, F1-Score
   - Matrice de confusion
   - Analyse des erreurs
   - **Interpr√©tation critique** dans un contexte m√©dical

---

## üõ† Outils et technologies

| Cat√©gorie | Technologies |
|-----------|--------------|
| **Langage** | Python 3.8+ |
| **Manipulation de donn√©es** | NumPy, Pandas |
| **Visualisation** | Matplotlib, Seaborn |
| **Machine Learning** | Scikit-learn |
| **Environnement** | Jupyter Notebook |
| **Contr√¥le de version** | Git, GitHub |

---

## üìÅ Structure du projet

```
building-perceptron/
‚îÇ
‚îú‚îÄ‚îÄ perceptron.py         # Classe Perceptron (POO)
‚îú‚îÄ‚îÄ notebook.ipynb        # Notebook complet (th√©orie + pratique)
‚îú‚îÄ‚îÄ README.md             # Documentation du projet
‚îú‚îÄ‚îÄ requirements.txt      # D√©pendances Python
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ README.md         # Explication : donn√©es charg√©es via sklearn
```

---

## üöÄ Installation et utilisation

### Pr√©requis
- Python 3.8 ou sup√©rieur
- pip

### Installation

```bash
# Cloner le repository
git clone https://github.com/[votre-username]/building-perceptron.git
cd building-perceptron

# Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les d√©pendances
pip install -r requirements.txt
```

### Utilisation

#### Tester le Perceptron sur donn√©es factices

```bash
python perceptron.py
```

#### Explorer le notebook complet

```bash
jupyter notebook notebook.ipynb
```

#### Utiliser la classe Perceptron dans votre code

```python
from perceptron import Perceptron
import numpy as np

# Cr√©er des donn√©es factices
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])  # AND logique

# Entra√Æner le perceptron
ppn = Perceptron(learning_rate=0.1, epochs=50, random_state=42)
ppn.fit(X, y)

# Pr√©dire
predictions = ppn.predict(X)
print(f"Pr√©dictions : {predictions}")
print(f"Accuracy : {ppn.score(X, y):.2%}")
```

---

## üìà R√©sultats

### Performance du mod√®le

| M√©trique | Train Set | Test Set |
|----------|-----------|----------|
| **Accuracy** | ~97% | ~96% |
| **Precision (Malignant)** | ~95% | ~94% |
| **Recall (Malignant)** | ~93% | ~92% |
| **F1-Score (Malignant)** | ~94% | ~93% |

*Note : Les valeurs exactes d√©pendent du split al√©atoire et de la variance de l'entra√Ænement*

### Observations cl√©s

‚úÖ **Points positifs** :
- Le Perceptron converge rapidement (< 50 √©poques)
- Performance globale satisfaisante (~96% accuracy)
- Bonne g√©n√©ralisation (√©cart train/test < 2%)
- Pr√©cision et recall √©quilibr√©s

‚ö†Ô∏è **Points d'attention** :
- Quelques faux n√©gatifs (tumeurs malignes non d√©tect√©es) : **critique en m√©dical**
- Sensibilit√© aux donn√©es non lin√©airement s√©parables
- Performance limit√©e par la nature lin√©aire du mod√®le

### Matrice de confusion (Test Set - valeurs approximatives)

|                | Pr√©dit Benign | Pr√©dit Malignant |
|----------------|---------------|------------------|
| **R√©el Benign** | 70 | 2 |
| **R√©el Malignant** | 3 | 39 |

**Interpr√©tation** :
- **Faux n√©gatifs (3)** : Tumeurs malignes class√©es comme b√©nignes ‚Üí **Risque majeur en m√©dical**
- **Faux positifs (2)** : Tumeurs b√©nignes class√©es comme malignes ‚Üí Stress et examens suppl√©mentaires

---

## ‚ö†Ô∏è Limites et am√©liorations

### Limites identifi√©es du Perceptron

1. **Lin√©arit√©** : Ne peut r√©soudre que des probl√®mes lin√©airement s√©parables
2. **Probl√®me XOR** : Incapable de r√©soudre XOR et autres probl√®mes non lin√©aires
3. **Fonction d'activation rigide** : Step function non diff√©rentiable
4. **Un seul neurone** : Capacit√© de repr√©sentation limit√©e
5. **Pas de probabilit√©s** : D√©cision binaire stricte sans confiance associ√©e

### Am√©liorations propos√©es

#### 1. **Perceptron Multi-Couches (MLP)**
```python
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(10, 5), 
                    activation='relu', 
                    max_iter=1000)
```
**Avantages** : Capture les relations non lin√©aires, plus de capacit√© de repr√©sentation

#### 2. **Support Vector Machine (SVM)**
```python
from sklearn.svm import SVC

svm = SVC(kernel='rbf', gamma='auto')
```
**Avantages** : Kernel trick pour g√©rer la non-lin√©arit√©, maximisation de la marge

#### 3. **Random Forest**
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=10)
```
**Avantages** : Robuste, g√®re la non-lin√©arit√©, fournit l'importance des features

#### 4. **R√©gression Logistique (soft perceptron)**
```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
```
**Avantages** : Probabilit√©s de classe, fonction d'activation diff√©rentiable

#### 5. **Gradient Boosting (XGBoost)**
```python
from xgboost import XGBClassifier

xgb = XGBClassifier(n_estimators=100, learning_rate=0.1)
```
**Avantages** : Souvent meilleures performances, robuste aux outliers

### Am√©liorations du workflow

- **Cross-validation** : Utiliser k-fold CV pour une estimation plus robuste des performances
- **Hyperparameter tuning** : GridSearchCV ou RandomizedSearchCV
- **Feature engineering** : Cr√©er des interactions entre features
- **Ensemble methods** : Combiner plusieurs mod√®les (voting, stacking)
- **Gestion du d√©s√©quilibre** : Class weights, SMOTE si n√©cessaire

---

## üìö Bibliographie

### Articles scientifiques et ressources techniques

1. **Rosenblatt, F.** (1958). *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain*. Psychological Review, 65(6), 386-408.

2. **Wolberg, W.H., Street, W.N., Mangasarian, O.L.** (1995). *Breast Cancer Wisconsin (Diagnostic) Data Set*. UCI Machine Learning Repository.

3. **Minsky, M., & Papert, S.** (1969). *Perceptrons: An Introduction to Computational Geometry*. MIT Press.

### Tutoriels et documentation

4. **Scikit-learn Documentation** - Breast Cancer Dataset  
   https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset

5. **Raschka, S.** - *Perceptron Algorithm with Code Example*  
   https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html

6. **3Blue1Brown** - *Neural Networks Series*  
   https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi

7. **OpenAI** - AI Experiments and Applications  
   https://openai.com/

8. **Google AI Experiments** - Quick, Draw!  
   https://quickdraw.withgoogle.com/

### Livres recommand√©s

9. **G√©ron, A.** (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media.

10. **Goodfellow, I., Bengio, Y., Courville, A.** (2016). *Deep Learning*. MIT Press.

---

## üë§ Auteur

**[Votre Nom]**  
Data Science Student  
[Votre Email] | [LinkedIn] | [GitHub]

---

## üìù Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

---

## üôè Remerciements

- Frank Rosenblatt pour l'invention du Perceptron
- La communaut√© scikit-learn pour les outils exceptionnels
- Les cr√©ateurs du dataset Breast Cancer Wisconsin
- Tous les contributeurs open-source qui rendent ces projets possibles

---

**‚≠ê Si ce projet vous a √©t√© utile, n'h√©sitez pas √† lui donner une √©toile !**
