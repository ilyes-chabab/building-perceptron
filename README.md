# Building Perceptron 

# Machine learning et deep learning

1) D√©finition
üîπ Machine Learning (ML)

Le Machine Learning est un domaine de l‚Äôintelligence artificielle o√π l‚Äôon entra√Æne un algorithme √† apprendre des relations √† partir de donn√©es plut√¥t que de programmer explicitement des r√®gles.

Au lieu de dire :

‚Äúsi pixel rouge + rond ‚Üí pomme‚Äù

on donne beaucoup d‚Äôexemples de pommes et de non-pommes, et l‚Äôalgorithme apprend lui-m√™me les r√®gles.

Id√©e cl√© :
‚û°Ô∏è L‚Äôhumain con√ßoit les caract√©ristiques importantes (features), la machine apprend les param√®tres.

Exemples d‚Äôalgorithmes ML :

r√©gression lin√©aire/logistique

k-nearest neighbors

arbres de d√©cision / random forest

SVM

clustering (k-means)

üîπ Deep Learning (DL)

Le Deep Learning est un sous-domaine du Machine Learning bas√© sur les r√©seaux de neurones profonds (deep neural networks).

Ici la machine apprend directement √† partir des donn√©es brutes sans que l‚Äôhumain d√©finisse les caract√©ristiques.

Exemple :

ML classique : on extrait contours, couleurs, textures d‚Äôune image

DL : on donne les pixels ‚Üí le r√©seau apprend lui-m√™me les contours

Id√©e cl√© :
‚û°Ô∏è La machine apprend les caract√©ristiques ET la d√©cision.

Architecture typique :

CNN (images)

RNN / LSTM (s√©quences)

Transformers (texte, audio, vision)

2) Quand utiliser l‚Äôun plut√¥t que l‚Äôautre ?
Utiliser le Machine Learning classique quand :

petit dataset

donn√©es tabulaires (Excel, base clients, scores)

besoin d‚Äôexplicabilit√© (banque, sant√©, assurance)

ressources mat√©rielles limit√©es

probl√®me simple de classification/pr√©diction

üìå Exemple :

pr√©dire si un client va r√©silier un abonnement

Utiliser le Deep Learning quand :

beaucoup de donn√©es

donn√©es complexes (image, audio, texte)

reconnaissance de motifs tr√®s difficiles

performance maximale recherch√©e

GPU disponible

üìå Exemple :

reconna√Ætre des visages ou comprendre du langage naturel

3) Applications du Deep Learning (3 exemples)
üß† 1. Vision par ordinateur (Computer Vision)

Le DL permet aux machines de voir et comprendre les images.

Applications :

d√©tection de tumeurs en radiologie

voitures autonomes

reconnaissance faciale

tri automatique d‚Äôobjets industriels

üëâ Les CNN analysent automatiquement les formes, textures et objets.

üó£Ô∏è 2. Traitement du langage naturel (NLP)

Les mod√®les de type Transformer (comme GPT) comprennent et g√©n√®rent du texte.

Applications :

assistants conversationnels

traduction automatique

r√©sum√© de documents

g√©n√©ration de code

üëâ La machine apprend la grammaire et le sens sans r√®gles √©crites.

üéµ 3. G√©n√©ration de contenu (IA g√©n√©rative)

Le Deep Learning peut cr√©er du contenu nouveau.

Applications :

g√©n√©ration d‚Äôimages

musique artificielle

voix synth√©tique r√©aliste

vid√©o g√©n√©r√©e par IA

üëâ Le mod√®le apprend la distribution des donn√©es et cr√©e de nouveaux exemples plausibles.

#  R√©ponses aux Questions 1 √† 7

## 1. Qu'est-ce qu'un Perceptron ? Quel est le lien avec un neurone biologique ?

### D√©finition

Le **Perceptron** est le premier mod√®le de neurone artificiel, invent√©
par **Frank Rosenblatt en 1957**.\
C'est un algorithme de **classification binaire supervis√©e** capable de
s√©parer deux classes √† l'aide d'une fronti√®re lin√©aire.

### Lien avec un neurone biologique

  Neurone biologique                  Perceptron
  ----------------------------------- ---------------------------
  Dendrites (re√ßoivent les signaux)   Entr√©es (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)
  Synapses (pond√®rent le signal)      Poids (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)
  Corps cellulaire                    Somme pond√©r√©e
  Potentiel d'activation              Fonction d'activation
  Axone (sortie)                      Sortie (y)

Le perceptron est une simplification math√©matique du fonctionnement d'un
neurone biologique.

------------------------------------------------------------------------

## 2. Fonction math√©matique du Perceptron et son usage

### Formule

y = f( Œ£ (w·µ¢ x·µ¢) + b )

### D√©finition des termes

-   x·µ¢ : variables d'entr√©e (features)
-   w·µ¢ : poids associ√©s aux entr√©es
-   b : biais (intercept)
-   Œ£ (w·µ¢ x·µ¢) : somme pond√©r√©e
-   f : fonction d'activation
-   y : sortie (classe pr√©dite)

### Usage

Le perceptron est utilis√© pour : - La classification binaire - Les
probl√®mes lin√©airement s√©parables - L'introduction aux r√©seaux de
neurones

------------------------------------------------------------------------

## 3. R√®gles d'apprentissage du Perceptron

### R√®gle de mise √† jour des poids

Si erreur :

w·µ¢ ‚Üê w·µ¢ + Œ∑ (y_true ‚àí y_pred) x·µ¢\
b ‚Üê b + Œ∑ (y_true ‚àí y_pred)

### D√©finitions

-   Œ∑ : taux d'apprentissage (learning rate)
-   y_true : vraie classe
-   y_pred : pr√©diction du mod√®le

Les poids sont ajust√©s uniquement si la pr√©diction est incorrecte.

------------------------------------------------------------------------

## 4. Fonction d'activation utilis√©e

Le perceptron classique utilise la **fonction seuil (fonction de
Heaviside)** :

-   1 si z ‚â• 0\
-   0 sinon

La sortie peut aussi √™tre cod√©e en {-1, +1}.

------------------------------------------------------------------------

## 5. Processus d'entra√Ænement du Perceptron

1.  Initialisation al√©atoire des poids\
2.  Pour chaque observation :
    -   Calcul de la somme pond√©r√©e
    -   Application de la fonction d'activation
    -   Comparaison avec la vraie classe
    -   Mise √† jour des poids si erreur
3.  R√©p√©tition sur plusieurs epochs\
4.  Arr√™t lorsque :
    -   plus d'erreur
    -   ou nombre maximal d'it√©rations atteint

Le perceptron converge uniquement si les donn√©es sont lin√©airement
s√©parables.

------------------------------------------------------------------------

## 6. Limites du Perceptron

-   Ne r√©sout pas les probl√®mes non lin√©aires (exemple : XOR)
-   Classification uniquement binaire (version de base)
-   Fronti√®re de d√©cision strictement lin√©aire
-   Sensible au choix du taux d'apprentissage

------------------------------------------------------------------------

## 7. D√©veloppement d'un Perceptron en Python (POO)

Pour d√©velopper un perceptron en programmation orient√©e objet :

-   Cr√©ation d'une classe `Perceptron`
-   M√©thodes principales :
    -   `__init__()` : initialisation des poids
    -   `fit()` : entra√Ænement
    -   `predict()` : pr√©diction
-   G√©n√©ration de donn√©es factices avec `numpy`
-   √âvaluation via accuracy

Ce d√©veloppement permet de comprendre : - Le fonctionnement interne d'un
mod√®le lin√©aire - L'impact des poids et du biais - Le m√©canisme
d'apprentissage supervis√©
